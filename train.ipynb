{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadData():\n",
    "    def __init__(self, data, num):\n",
    "        self.data = data\n",
    "        self.data_list = []\n",
    "        self.num = num\n",
    "\n",
    "    def split_data(self):\n",
    "        with open(self.data, 'r', encoding='utf-8') as f:\n",
    "            for item in jsonlines.Reader(f):\n",
    "                self.data_list.append(item)\n",
    "        datas, labels = [], []\n",
    "        for data in self.data_list[:self.num]:\n",
    "            if data['label'] >= 0:\n",
    "                datas.append(data)\n",
    "                labels.append(data['label'])\n",
    "        return datas, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wash(x):\n",
    "    return [str(i) for i in x]\n",
    "\n",
    "class FeatureExtractor():\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "\n",
    "    def byteentropy(self):\n",
    "        return np.array([text['byteentropy'] for text in self.texts])\n",
    "\n",
    "    def histogram(self):\n",
    "        return np.array([text['histogram'] for text in self.texts])\n",
    "\n",
    "    def hash256(self, items):\n",
    "        all_hash256 = []\n",
    "        for item in items:\n",
    "            hash256 = [0]*256\n",
    "            for name in wash(item):\n",
    "                hash_value = int(hashlib.sha256(name.encode()).hexdigest(), 16) % 256\n",
    "                hash256[hash_value] += 1\n",
    "            all_hash256.append(hash256)\n",
    "        return np.array(all_hash256)\n",
    "\n",
    "    def imports_hash(self):\n",
    "        return self.hash256([text['imports'] for text in self.texts])\n",
    "\n",
    "    def header_hash(self):\n",
    "        return self.hash256([text['header'] for text in self.texts])\n",
    "\n",
    "    def get_features(self):\n",
    "        entropy = self.byteentropy()\n",
    "        histogram = self.histogram()\n",
    "        imports = self.imports_hash()\n",
    "        header = self.header_hash()\n",
    "        features = np.concatenate([entropy, imports, histogram, header], axis=1)\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MalwareClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MalwareClassifier, self).__init__()\n",
    "        \n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=1, kernel_size=1, stride=1, padding=0),\n",
    "            nn.Identity() \n",
    "        )\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_size=input_dim, hidden_size=input_dim, num_layers=1, batch_first=True)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.PReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.PReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(512, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.log10(1 + x)\n",
    "        \n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.cnn(x) \n",
    "        x = x.squeeze(1)\n",
    "        \n",
    "\n",
    "        x = x.unsqueeze(1)  \n",
    "        x, _ = self.rnn(x) \n",
    "        x = x.squeeze(1)  \n",
    "        \n",
    "   \n",
    "        logits = self.model(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, labels = ReadData('./ember_dataset/train_features_1.jsonl', 5000).split_data()\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征维度: (3185, 1024)\n"
     ]
    }
   ],
   "source": [
    "extractor = FeatureExtractor(texts)\n",
    "features = extractor.get_features()\n",
    "print(\"特征维度:\", features.shape)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, labels, test_size=0.2, random_state=42, stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "model = MalwareClassifier(input_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MalwareClassifier(\n",
       "  (cnn): Sequential(\n",
       "    (0): Conv1d(1, 1, kernel_size=(1,), stride=(1,))\n",
       "    (1): Identity()\n",
       "  )\n",
       "  (rnn): LSTM(1024, 1024, batch_first=True)\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Dropout(p=0.3, inplace=False)\n",
       "    (4): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): PReLU(num_parameters=1)\n",
       "    (7): Dropout(p=0.3, inplace=False)\n",
       "    (8): Linear(in_features=512, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv1d) or isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.BatchNorm1d):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "model.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "epochs = 100\n",
    "batch_size = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 1.1780\n",
      "Epoch [2/100], Loss: 0.9537\n",
      "Epoch [3/100], Loss: 0.8077\n",
      "Epoch [4/100], Loss: 0.7266\n",
      "Epoch [5/100], Loss: 0.6851\n",
      "Epoch [6/100], Loss: 0.6240\n",
      "Epoch [7/100], Loss: 0.5828\n",
      "Epoch [8/100], Loss: 0.5488\n",
      "Epoch [9/100], Loss: 0.5262\n",
      "Epoch [10/100], Loss: 0.4820\n",
      "Epoch [11/100], Loss: 0.4879\n",
      "Epoch [12/100], Loss: 0.4311\n",
      "Epoch [13/100], Loss: 0.4117\n",
      "Epoch [14/100], Loss: 0.4051\n",
      "Epoch [15/100], Loss: 0.3677\n",
      "Epoch [16/100], Loss: 0.3631\n",
      "Epoch [17/100], Loss: 0.3524\n",
      "Epoch [18/100], Loss: 0.3232\n",
      "Epoch [19/100], Loss: 0.3235\n",
      "Epoch [20/100], Loss: 0.3091\n",
      "Epoch [21/100], Loss: 0.2836\n",
      "Epoch [22/100], Loss: 0.2902\n",
      "Epoch [23/100], Loss: 0.2745\n",
      "Epoch [24/100], Loss: 0.2696\n",
      "Epoch [25/100], Loss: 0.2517\n",
      "Epoch [26/100], Loss: 0.2535\n",
      "Epoch [27/100], Loss: 0.2442\n",
      "Epoch [28/100], Loss: 0.2462\n",
      "Epoch [29/100], Loss: 0.2450\n",
      "Epoch [30/100], Loss: 0.2184\n",
      "Epoch [31/100], Loss: 0.2172\n",
      "Epoch [32/100], Loss: 0.2248\n",
      "Epoch [33/100], Loss: 0.2244\n",
      "Epoch [34/100], Loss: 0.2050\n",
      "Epoch [35/100], Loss: 0.1973\n",
      "Epoch [36/100], Loss: 0.2059\n",
      "Epoch [37/100], Loss: 0.1965\n",
      "Epoch [38/100], Loss: 0.1994\n",
      "Epoch [39/100], Loss: 0.1859\n",
      "Epoch [40/100], Loss: 0.1784\n",
      "Epoch [41/100], Loss: 0.1780\n",
      "Epoch [42/100], Loss: 0.1872\n",
      "Epoch [43/100], Loss: 0.1691\n",
      "Epoch [44/100], Loss: 0.1724\n",
      "Epoch [45/100], Loss: 0.1709\n",
      "Epoch [46/100], Loss: 0.1589\n",
      "Epoch [47/100], Loss: 0.1593\n",
      "Epoch [48/100], Loss: 0.1559\n",
      "Epoch [49/100], Loss: 0.1435\n",
      "Epoch [50/100], Loss: 0.1427\n",
      "Epoch [51/100], Loss: 0.1416\n",
      "Epoch [52/100], Loss: 0.1435\n",
      "Epoch [53/100], Loss: 0.1415\n",
      "Epoch [54/100], Loss: 0.1403\n",
      "Epoch [55/100], Loss: 0.1290\n",
      "Epoch [56/100], Loss: 0.1271\n",
      "Epoch [57/100], Loss: 0.1353\n",
      "Epoch [58/100], Loss: 0.1306\n",
      "Epoch [59/100], Loss: 0.1218\n",
      "Epoch [60/100], Loss: 0.1249\n",
      "Epoch [61/100], Loss: 0.1245\n",
      "Epoch [62/100], Loss: 0.1156\n",
      "Epoch [63/100], Loss: 0.1155\n",
      "Epoch [64/100], Loss: 0.1107\n",
      "Epoch [65/100], Loss: 0.1126\n",
      "Epoch [66/100], Loss: 0.1108\n",
      "Epoch [67/100], Loss: 0.0975\n",
      "Epoch [68/100], Loss: 0.1075\n",
      "Epoch [69/100], Loss: 0.1070\n",
      "Epoch [70/100], Loss: 0.1011\n",
      "Epoch [71/100], Loss: 0.0954\n",
      "Epoch [72/100], Loss: 0.0999\n",
      "Epoch [73/100], Loss: 0.0942\n",
      "Epoch [74/100], Loss: 0.0896\n",
      "Epoch [75/100], Loss: 0.0887\n",
      "Epoch [76/100], Loss: 0.0900\n",
      "Epoch [77/100], Loss: 0.0936\n",
      "Epoch [78/100], Loss: 0.0848\n",
      "Epoch [79/100], Loss: 0.0870\n",
      "Epoch [80/100], Loss: 0.0808\n",
      "Epoch [81/100], Loss: 0.0851\n",
      "Epoch [82/100], Loss: 0.0808\n",
      "Epoch [83/100], Loss: 0.0747\n",
      "Epoch [84/100], Loss: 0.0776\n",
      "Epoch [85/100], Loss: 0.0782\n",
      "Epoch [86/100], Loss: 0.0799\n",
      "Epoch [87/100], Loss: 0.0768\n",
      "Epoch [88/100], Loss: 0.0735\n",
      "Epoch [89/100], Loss: 0.0706\n",
      "Epoch [90/100], Loss: 0.0654\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    permutation = torch.randperm(X_train.size()[0])\n",
    "    epoch_loss = 0\n",
    "    for i in range(0, X_train.size()[0], batch_size):\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_x, batch_y = X_train[indices], y_train[indices]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / (X_train.size()[0] // batch_size)\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9451\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9279    0.9156    0.9217       225\n",
      "           1     0.9542    0.9612    0.9577       412\n",
      "\n",
      "    accuracy                         0.9451       637\n",
      "   macro avg     0.9411    0.9384    0.9397       637\n",
      "weighted avg     0.9449    0.9451    0.9450       637\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # 直接在 GPU 上进行推理\n",
    "    outputs = model(X_test)  # X_test 已经在 GPU 上\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    # 将结果移回 CPU 并转换为 numpy 数组\n",
    "    y_test_cpu = y_test.cpu().numpy()\n",
    "    predicted_cpu = predicted.cpu().numpy()\n",
    "    \n",
    "    # 计算准确率和分类报告\n",
    "    accuracy = accuracy_score(y_test_cpu, predicted_cpu)\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')\n",
    "    print(classification_report(y_test_cpu, predicted_cpu, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cpu = model.to('cpu')\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model_cpu.state_dict(), 'malware_classifier_cpu.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
